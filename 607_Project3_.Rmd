---
title: "Projcet3_607"
author: "Tyler, Noah Collin, Shane Hylton"
date: "10/16/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Read in CSVs (which came from postgres Relational Database)

## DataAnaylst.csv had 47 "problems" to start
```{r read in DataAnalyst.csv}
DataAnaylstsRaw<-read.csv("DataAnalyst.csv", na.strings = "")
glimpse(DataAnaylstsRaw)
head(DataAnaylstsRaw)
problems(DataAnaylstsRaw)
names(DataAnaylstsRaw)
```


```{r read in DataScientist.csv}
DataScientistRaw <- read_csv("DataScientist.csv")
glimpse(DataScientistRaw)
head(DataScientistRaw)
problems(DataAnaylstsRaw)
names(DataScientistRaw)
```


```{r read in alldata.csv}
allData <- read_csv("alldata.csv")
glimpse(allData)
head(allData)
problems(allData)
```

```{r combine2datasets}

Analysts <- subset(DataAnaylstsRaw,select =  (c("Job.Title", "Salary.Estimate","Job.Description")) )

Scientists <- subset(DataScientistRaw, select = c("Job Title","Salary Estimate","Job Description"))

colnames(Scientists) <- (c("Job.Title", "Salary.Estimate","Job.Description"))

combinedData <- rbind(Analysts, Scientists)

```

```{r CreateMinMaxAndMean columns}


Sals <- (combinedData$Salary.Estimate)


(minsals <- str_extract(Sals,"\\d\\d+"))

minsals <-  as.numeric(minsals)

(maxSals <- str_extract(Sals, "\\-\\$(\\d\\d+)"))

maxSals <- str_extract(maxSals, "\\d\\d+") 

maxSals <- as.numeric(maxSals)


combinedData$MinSalaries <-  minsals

combinedData$MaxSalaries <-  maxSals

sum(is.na (combinedData$MaxSalaries)) 
sum(is.na (combinedData$MinSalaries)) 

combinedData$MeanSalary <- rowMeans(combinedData[,c("MinSalaries", "MaxSalaries")], na.rm = TRUE)

#combinedData$MeanSalary <- (combinedData$MaxSalaries+ combinedData$MinSalaries)/2



sum(is.na (combinedData$MeanSalary)) 

combinedData <- drop_na(combinedData)

#See ranges of mean Salaries
range(combinedData$MeanSalary)

```
```{r SalaryBins}

OneHundredToTwoHundredK <- combinedData %>% filter(combinedData$MeanSalary>=100 & combinedData$MeanSalary<=200)

range(SixFigues$MeanSalary)

# per https://policyadvice.net/insurance/insights/average-american-income/
LessThanUSMeanSalary <- combinedData %>% filter(combinedData$MeanSalary < 51.9)

UpperEchelon <- combinedData %>% filter(combinedData$MeanSalary > 200)
```


```{r wordCloud-UE}
#library(ggplot2)
#install.packages("ggwordcloud")
#library(ggwordcloud)

description <- data.frame(UpperEchelon$Job.Description)
desclist <- unlist(description)
desclist <- str_remove_all(desclist, '[[:punct:]]')
desclist <- str_remove_all(desclist, '[\r\n]')
desclist <- str_remove_all(desclist, '[0-9]')
desclist <- str_remove_all(desclist, '[\r\n]')
#desclist <- str_replace_all(desclist, '[+>~`^-_|]', " ")
desclist <- tolower(desclist)
descsplit <- strsplit(desclist, " ")

frequenciesUE <- table(unlist(descsplit))

frequenciesUE <- data.frame(frequenciesUE)

colnames(frequenciesUE) <- c('word', 'count')

omit <- c(" ", "and", "with", "from", "for", "the", "our", "your", "are", "will", "with", "that", "other", "all", "have", "to", "of", "", "this", "you", "a", "in", "is", "or", "as", "on", "be", "we", "by", "at", "an", "their", "us", "it", "can", "who", "such", "through", "into", "including")

#get rid of BS words
frequenciesUE <- subset(frequenciesUE, as.numeric(count) >= 3)

# Get rid of prepoistions, etc
frequenciesUE_relevant <- frequenciesUE[!frequenciesUE$word %in% omit, ]

#Sorting most frequent to least frequent, hyphen to sort DESCENDING
frequenciesUE_relevant <- frequenciesUE_relevant[order(-frequenciesUE_relevant$count), ]

#Plotting
overweighted <- c("data", "analyst", "analysis", "analytics")


freqplot <- frequenciesUE_relevant[!frequenciesUE_relevant$word %in% overweighted, ]

freqplot <- freqplot[1:50, ]

ggplot(freqplot, aes(label = word, size = count, color = factor(sample.int(8, nrow(freqplot), replace = TRUE))))+
  geom_text_wordcloud()+
  scale_radius(range = c(0, 12), limits = c(0, NA)) +
  theme_minimal()+
  ggtitle("Upper Echelon ($200k+ salaries) job decriptions, top 50") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r wordCloud-UE}
library(ggplot2)
#install.packages("ggwordcloud")
library(ggwordcloud)

description <- data.frame(LessThanUSMeanSalary$Job.Description)
desclist <- unlist(description)
desclist <- str_remove_all(desclist, '[[:punct:]]')
desclist <- str_remove_all(desclist, '[\r\n]')
desclist <- str_remove_all(desclist, '[0-9]')
desclist <- str_remove_all(desclist, '[\r\n]')
#desclist <- str_replace_all(desclist, '[+>~`^-_|]', " ")
desclist <- tolower(desclist)
descsplit <- strsplit(desclist, " ")

frequenciesUSMean <- table(unlist(descsplit))

frequenciesUSMean <- data.frame(frequenciesUSMean)

colnames(frequenciesUSMean) <- c('word', 'count')

#get rid of BS words
frequenciesUSMean <- subset(frequenciesUSMean, as.numeric(count) >= 3)

# Get rid of prepoistions, etc
frequenciesUSMean_relevant <- frequenciesUSMean[!frequenciesUSMean$word %in% omit, ]

#Sorting most frequent to least frequent, hyphen to sort DESCENDING
frequenciesUSMean_relevant <- frequenciesUSMean_relevant[order(-frequenciesUSMean_relevant$count), ]

#Plotting


freqplot <- frequenciesUSMean_relevant[!frequenciesUSMean_relevant$word %in% overweighted, ]

freqplot <- freqplot[1:50, ]

ggplot(freqplot, aes(label = word, size = count, color = factor(sample.int(8, nrow(freqplot), replace = TRUE))))+
  geom_text_wordcloud()+
  scale_radius(range = c(0, 12), limits = c(0, NA)) +
  theme_minimal()+
  ggtitle("Data Job descriptions paying less than the US Mean (~$51K), top 50 words") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r wordCloud-UE}
#library(ggplot2)
#install.packages("ggwordcloud")
#library(ggwordcloud)

description <- data.frame(OneHundredToTwoHundredK$Job.Description)
desclist <- unlist(description)
desclist <- str_remove_all(desclist, '[[:punct:]]')
desclist <- str_remove_all(desclist, '[\r\n]')
desclist <- str_remove_all(desclist, '[0-9]')
desclist <- str_remove_all(desclist, '[\r\n]')
#desclist <- str_replace_all(desclist, '[+>~`^-_|]', " ")
desclist <- tolower(desclist)
descsplit <- strsplit(desclist, " ")

frequencies100K200K <- table(unlist(descsplit))

frequencies100K200K <- data.frame(frequencies100K200K)

colnames(frequencies100K200K) <- c('word', 'count')

#get rid of BS words
frequencies100K200K <- subset(frequencies100K200K, as.numeric(count) >= 3)

# Get rid of prepoistions, etc
frequencies100K200K_relevant <- frequencies100K200K[!frequencies100K200K$word %in% omit, ]

#Sorting most frequent to least frequent, hyphen to sort DESCENDING
frequencies100K200K_relevant <- frequencies100K200K_relevant[order(-frequencies100K200K_relevant$count), ]

#Plotting
freqplot <- frequencies100K200K_relevant[!frequencies100K200K_relevant$word %in% overweighted, ]

freqplot <- freqplot[1:50, ]

ggplot(freqplot, aes(label = word, size = count, color = factor(sample.int(8, nrow(freqplot), replace = TRUE))))+
  geom_text_wordcloud()+
  scale_radius(range = c(0, 12), limits = c(0, NA)) +
  theme_minimal()+
  ggtitle("Data Jobs paying $100k-$200k, job decriptions, top 50 words") +
  theme(plot.title = element_text(hjust = 0.5))
```

